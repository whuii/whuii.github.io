---
title: "A federated anti-forgetting representation method based on hybrid model architecture and gradient truncation"
collection: publications
category: manuscripts
permalink:
excerpt: '**Abstract**: Unsupervised Federated Continual Learning (UFCL) is a new learning paradigm that embeds unsupervised representation techniques into the Federated Learning (FL) framework, which enables continuous training of a shared representation model without compromising individual participants’ data privacy. However, the continuous learning process may cause catastrophic forgetting in the model, reducing generated representations’ performance.
Our research findings suggest that limited model capacity and undifferentiated weight aggregation in UFCL are mainly responsible for decreased model performance. Therefore, this paper proposes an anti-forgetting representation learning method based on a new hybrid model architecture and gradient truncation technique, namely FedAFR. The contributions can be summarized as follows. (1) We propose a model architecture based on Kolmogorov-Arnold and pluggable structures, which can effectively improve the model’s memory capacity and anti-forgetting ability. (2) We design a gradient truncation technique to reduce the interference of weight aggregation on model memory and use an ordinary differential equation (ODE) sampler to augment the representation performance. (3) We carry out experiments to compare FedAFR against the state-of-the-art representation methods in FL.
date: Aug. 2025
venue: 'Volume 19'
venue: 'Frontiers of Computer Science(FCS)'
paperurl: 'http://whuii.github.io/files/fedafr-fcs2025.pdf'
citation: 'Hui WANG, Jie SUN, Tianyu WO, Xudong LIU, Suzhen PEI. A federated anti-forgetting representation method based on hybrid model architecture and gradient truncation. Front. Comput. Sci., 2025, 19(6): 196339 https://doi.org/10.1007/s11704-024-40557-w'
---

**Abstract**: Unsupervised Federated Continual Learning (UFCL) is a new learning paradigm that embeds unsupervised representation techniques into the Federated Learning (FL) framework, which enables continuous training of a shared representation model without compromising individual participants’ data privacy. However, the continuous learning process may cause catastrophic forgetting in the model, reducing generated representations’ performance.
Our research findings suggest that limited model capacity and undifferentiated weight aggregation in UFCL are mainly responsible for decreased model performance. Therefore, this paper proposes an anti-forgetting representation learning method based on a new hybrid model architecture and gradient truncation technique, namely FedAFR. The contributions can be summarized as follows. (1) We propose a model architecture based on Kolmogorov-Arnold and pluggable structures, which can effectively improve the model’s memory capacity and anti-forgetting ability. (2) We design a gradient truncation technique to reduce the interference of weight aggregation on model memory and use an ordinary differential equation (ODE) sampler to augment the representation performance. (3) We carry out experiments to compare FedAFR against the state-of-the-art representation methods in FL.

